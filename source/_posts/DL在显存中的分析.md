---
title: Deep Learning的模型在GPU显存中的分析
date: 2020-02-22
tags: install
category: [deeplearning,GPU]
---

# 神经网络显存占用
神经网络模型占用的显存主要包含：
1. 模型自身的参数
2. 模型的输出
## 单层DNN网络
由`Y=XW`组成

那么该模型显存占用包括：
1. 参数：二维矩阵`W`
2. 模型的输出：二维矩阵`Y`

### 参数的显存占用
只有有参数的层，才会有显存占用。这部份的显存占用和**输入无关**，模型加载完成之后就会占用

更具体的来说，模型的参数数目(这里均不考虑偏置项b)为：

> 1. Linear(M->N): 参数数目：M×N
> 2. Conv2d(in, out, K): 参数数目：in × out × K × K
> 3. BatchNorm(N): 参数数目： 2N
> 4. Embedding(N,W): 参数数目： N × W

**参数占用显存 = 参数数目×`n`**
```
n = 4 ：float32
n = 2 : float16
n = 8 : double64
```
在PyTorch中，当你执行完model=MyGreatModel().cuda()之后就会占用相应的显存，占用的显存大小基本与上述分析的显存差不多（会稍大一些，因为其它开销）。
### 梯度与动量的显存占用
普通SGD还有梯度值保存，所以显存占用等于参数占用的显存x2
如果有动量的SGD，还想保存动量，所以显存占用等于参数占用的显存x3

总结一下，模型中与**输入无关**的显存占用包括：
> 1. 参数 W
> 2. 梯度 dW（一般与参数一样）
> 3. 优化器的动量（普通SGD没有动量，momentum-SGD动量与梯度一样，Adam优化器动量的数量是梯度的两倍）

### 输入输出的显存占用
模型输出的显存占用，总结如下：
> 1. 需要计算每一层的feature map的形状（多维数组的形状）
> 2. 需要保存输出对应的梯度用以反向传播（链式法则）
> 3. 显存占用与 batch size 成正比
> 4. 模型输出不需要存储相应的动量信息。

深度学习中神经网络的显存占用，我们可以得到如下公式：
```
显存占用 = 模型显存占用 + batch_size × 每个样本的显存占用
```
可以看出显存不是和batch-size简单的成正比，尤其是模型自身比较复杂的情况下：比如全连接很大，Embedding层很大
